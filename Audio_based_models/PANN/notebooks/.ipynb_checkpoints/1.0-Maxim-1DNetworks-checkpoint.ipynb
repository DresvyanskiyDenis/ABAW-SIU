{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "import re\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.notebooks_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_root = '/media/maxim/SStorage/FG_2020/'\n",
    "features_root = 'features/'\n",
    "labels_path = ''\n",
    "log_root = '../logs/'\n",
    "tb_log_root = '../logs/tb/'\n",
    "\n",
    "features_name = 'mfcc_30_0-2'\n",
    "\n",
    "df_labels = pd.read_csv(os.path.join(data_root, labels_path))\n",
    "\n",
    "class_names = list(map(str, df_labels['label'].unique()))\n",
    "class_names.sort()\n",
    "\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(data_root, features_root, 'agender.{0}.train.pickle'.format(features_name)), 'rb') as f:\n",
    "    x_train = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(data_root, features_root, 'agender.{0}.valid.pickle'.format(features_name)), 'rb') as f:\n",
    "    x_valid = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(data_root, features_root, 'agender.{0}.test.pickle'.format(features_name)), 'rb') as f:\n",
    "    x_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "class AddPad(object):\n",
    "    def __call__(self, x):\n",
    "        x = F.pad(x, pad=(0, abs(196 - x.shape[1])), mode='constant', value=0)\n",
    "        return x\n",
    "    \n",
    "addPad = AddPad()\n",
    "\n",
    "train_transforms = transforms.Compose([addPad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "define_seed(12)\n",
    "\n",
    "x_train = torch.Tensor(x_train)\n",
    "x_valid = torch.Tensor(x_valid)\n",
    "x_test = torch.Tensor(x_test)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = torch.LongTensor(le.fit_transform(df_labels['label'][df_labels['file_name'].str.startswith('train')].values))\n",
    "y_valid = torch.LongTensor(le.transform(df_labels['label'][df_labels['file_name'].str.startswith('valid')].values))\n",
    "y_test = torch.LongTensor(le.transform(df_labels['label'][df_labels['file_name'].str.startswith('test')].values))\n",
    "\n",
    "train_dataset = CustomTensorDataset(tensors=(x_train, y_train), transform=train_transforms)\n",
    "valid_dataset = CustomTensorDataset(tensors=(x_valid, y_valid), transform=train_transforms)\n",
    "test_dataset = CustomTensorDataset(tensors=(x_test, y_test), transform=train_transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                               shuffle=True, num_workers=6)\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, \n",
    "                                               shuffle=False, num_workers=6)\n",
    "\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, \n",
    "#                                               shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "class StatPool(nn.Module):\n",
    "    def __init__(self, dimension, keepdim):\n",
    "        super(StatPool, self).__init__()\n",
    "        self.dimension = dimension\n",
    "        self.keepdim = keepdim\n",
    "\n",
    "    def forward(self, x):\n",
    "        var = x.var(dim=self.dimension, keepdim=self.keepdim)\n",
    "        mean = x.mean(dim=self.dimension, keepdim=self.keepdim)\n",
    "        return torch.cat([var, mean], dim=1)\n",
    "    \n",
    "class AgenderTDNNv1(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(AgenderTDNNv1, self).__init__()\n",
    "        self.feat_extr = nn.Sequential(\n",
    "            nn.Conv1d(num_inputs, 512, 5, padding=2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(512, 512, 3, dilation=2, padding=2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(512, 512, 3, dilation=3, padding=3),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(512, 512, 1), \n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(512, 1500, 1), \n",
    "            nn.BatchNorm1d(1500),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.6)\n",
    "        )\n",
    "        \n",
    "        self.z1 = nn.Conv1d(num_inputs, 512, 5, padding=2)\n",
    "        self.z2 = nn.BatchNorm1d(512)\n",
    "        self.z3 = nn.ReLU(inplace=True)\n",
    "        self.z4 = nn.Conv1d(512, 512, 3, dilation=2, padding=2)\n",
    "        self.z5 = nn.BatchNorm1d(512)\n",
    "        self.z6 = nn.ReLU(inplace=True)\n",
    "        self.z7 = nn.Conv1d(512, 512, 3, dilation=3, padding=3)\n",
    "        self.z8 = nn.BatchNorm1d(512)\n",
    "        self.z9 = nn.ReLU(inplace=True)\n",
    "        self.z10 = nn.Conv1d(512, 512, 1)\n",
    "        self.z11 = nn.BatchNorm1d(512)\n",
    "        self.z12 = nn.ReLU(inplace=True)\n",
    "        self.z13 = nn.Conv1d(512, 1500, 1)\n",
    "        self.z14 = nn.BatchNorm1d(1500)\n",
    "        self.z15 = nn.ReLU(inplace=True)\n",
    "        self.z16 = nn.Dropout(p=0.6)\n",
    "        \n",
    "        self.stp = StatPool(dimension=-1, keepdim=False)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(3000, 400),\n",
    "            nn.BatchNorm1d(400),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(400, num_outputs)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "#         x = self.feat_extr(x)\n",
    "        x = self.z1(x)\n",
    "        print(x.shape)\n",
    "        x = self.z2(x)\n",
    "        print(x.shape)\n",
    "        x = self.z3(x)\n",
    "        print(x.shape)\n",
    "        x = self.z4(x)\n",
    "        print(x.shape)\n",
    "        x = self.z5(x)\n",
    "        print(x.shape)\n",
    "        x = self.z6(x)\n",
    "        print(x.shape)\n",
    "        x = self.z7(x)\n",
    "        print(x.shape)\n",
    "        x = self.z8(x)\n",
    "        print(x.shape)\n",
    "        x = self.z9(x)\n",
    "        print(x.shape)\n",
    "        x = self.z10(x)\n",
    "        print(x.shape)\n",
    "        x = self.z11(x)\n",
    "        print(x.shape)\n",
    "        x = self.z12(x)\n",
    "        print(x.shape)\n",
    "        x = self.z13(x)\n",
    "        print(x.shape)\n",
    "        x = self.z14(x)\n",
    "        print(x.shape)\n",
    "        x = self.z15(x)\n",
    "        print(x.shape)\n",
    "        x = self.z16(x)\n",
    "        print(x.shape)\n",
    "        x = self.stp(x)\n",
    "        print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv1d, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, **kwargs)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "    \n",
    "class AgenderCNN(nn.Module):\n",
    "    def __init__(self, num_inputs, out_size):\n",
    "        super(AgenderCNN, self).__init__()      \n",
    "        self.feat_extr = nn.Sequential(\n",
    "            BasicConv1d(num_inputs, 128, kernel_size=3, padding=1, stride=1),\n",
    "            BasicConv1d(128, 128, kernel_size=3, padding=1, stride=1),\n",
    "            BasicConv1d(128, 128, kernel_size=5, padding=1, stride=1),\n",
    "            nn.Dropout(p=0.15),\n",
    "            BasicConv1d(128, 128, kernel_size=3, padding=1, stride=1),\n",
    "            BasicConv1d(128, 256, kernel_size=1, padding=1, stride=1)\n",
    "        )\n",
    "        \n",
    "        self.stp = StatPool(dimension=-1, keepdim=False)\n",
    "\n",
    "        self.classifier = nn.Linear(in_features=512, out_features=out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feat_extr(x)\n",
    "        x = self.stp(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "class TDNN(nn.Module):\n",
    "    def __init__(self, input_dim=23, output_dim=512, context_size=5, stride=1, dilation=1,\n",
    "                 batch_norm=True, dropout_p=0.0):\n",
    "        \"\"\"\n",
    "        TDNN as defined by https://www.danielpovey.com/files/2015_interspeech_multisplice.pdf\n",
    "        Affine transformation not applied globally to all frames but smaller windows with local context\n",
    "        batch_norm: True to include batch normalisation after the non linearity\n",
    "\n",
    "        Context size and dilation determine the frames selected\n",
    "        (although context size is not really defined in the traditional sense)\n",
    "        For example:\n",
    "            context size 5 and dilation 1 is equivalent to [-2,-1,0,1,2]\n",
    "            context size 3 and dilation 2 is equivalent to [-2, 0, 2]\n",
    "            context size 1 and dilation 1 is equivalent to [0]\n",
    "        \"\"\"\n",
    "        super(TDNN, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.stride = stride\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dilation = dilation\n",
    "        self.dropout_p = dropout_p\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.kernel = nn.Linear(input_dim * context_size, output_dim)\n",
    "        self.nonLinearity = nn.ReLU()\n",
    "        if self.batch_norm:\n",
    "            self.bn = nn.BatchNorm1d(output_dim)\n",
    "        if self.dropout_p:\n",
    "            self.drop = nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input: size (batch, seq_len, input_features)\n",
    "        output: size (batch, new_seq_len, output_features)\n",
    "        \"\"\"\n",
    "        _, _, d = x.shape\n",
    "        assert (d == self.input_dim), 'Input dimension was wrong. Expected ({}), got ({})'.format(self.input_dim, d)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Unfold input into smaller temporal contexts\n",
    "        x = F.unfold(\n",
    "            x,\n",
    "            (self.context_size, self.input_dim),\n",
    "            stride=(1, self.input_dim),\n",
    "            dilation=(self.dilation, 1)\n",
    "        )\n",
    "\n",
    "        # N, output_dim*context_size, new_t = x.shape\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.kernel(x)\n",
    "        x = self.nonLinearity(x)\n",
    "\n",
    "        if self.dropout_p:\n",
    "            x = self.drop(x)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "class AgenderTDNNv0(nn.Module):\n",
    "    def __init__(self, num_inputs, out_size):\n",
    "        # Input to frame1 is of shape (batch_size, T, 24)\n",
    "        # Output of frame5 will be (batch_size, T-14, 1500)\n",
    "        # -> CONV/FC -> BatchNorm -> ReLu(or other activation) -> Dropout -> CONV/FC ->\n",
    "        super(AgenderTDNNv0, self).__init__()\n",
    "\n",
    "        self.feat_extr = nn.Sequential(\n",
    "            TDNN(input_dim=num_inputs, output_dim=512, context_size=5, dilation=1),\n",
    "            TDNN(input_dim=512, output_dim=512, context_size=3, dilation=2),\n",
    "            TDNN(input_dim=512, output_dim=512, context_size=3, dilation=3),\n",
    "            TDNN(input_dim=512, output_dim=512, context_size=1, dilation=1),\n",
    "            TDNN(input_dim=512, output_dim=1500, context_size=1, dilation=1),\n",
    "            nn.Dropout(p=0.6)\n",
    "        )\n",
    "        \n",
    "        self.z1 = TDNN(input_dim=num_inputs, output_dim=512, context_size=5, dilation=1)\n",
    "        self.z2 = TDNN(input_dim=512, output_dim=512, context_size=3, dilation=2)\n",
    "        self.z3 = TDNN(input_dim=512, output_dim=512, context_size=3, dilation=3)\n",
    "        self.z4 = TDNN(input_dim=512, output_dim=512, context_size=1, dilation=1)\n",
    "        self.z5 = TDNN(input_dim=512, output_dim=1500, context_size=1, dilation=1)\n",
    "        self.z6 = nn.Dropout(p=0.6)\n",
    "\n",
    "        self.stp = StatPool(dimension=1, keepdim=False)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=3000, out_features=400),\n",
    "            nn.BatchNorm1d(400),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(in_features=400, out_features=400),\n",
    "            nn.BatchNorm1d(400),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(in_features=400, out_features=out_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(x.shape)\n",
    "#         x = self.feat_extr(x)\n",
    "        x = self.z1(x)\n",
    "        print(x.shape)\n",
    "        x = self.z2(x)\n",
    "        print(x.shape)\n",
    "        x = self.z3(x)\n",
    "        print(x.shape)\n",
    "        x = self.z4(x)\n",
    "        print(x.shape)\n",
    "        x = self.z5(x)\n",
    "        print(x.shape)\n",
    "        x = self.z6(x)\n",
    "\n",
    "        print(x.shape)\n",
    "        x = self.stp(x)\n",
    "        print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 90, 196])\n",
      "torch.Size([2, 196, 90])\n",
      "torch.Size([2, 192, 512])\n",
      "torch.Size([2, 188, 512])\n",
      "torch.Size([2, 182, 512])\n",
      "torch.Size([2, 182, 512])\n",
      "torch.Size([2, 182, 1500])\n",
      "torch.Size([2, 182, 1500])\n",
      "torch.Size([2, 3000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1230, -0.5362, -0.1084,  0.2549],\n",
       "        [-0.5441,  0.1496, -0.7393,  0.3533]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgenderTDNNv0(90, 4)(torch.Tensor([x_train[0], x_train[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 90, 196])\n",
      "torch.Size([2, 512, 196])\n",
      "torch.Size([2, 512, 196])\n",
      "torch.Size([2, 512, 196])\n",
      "torch.Size([2, 512, 196])\n",
      "torch.Size([2, 512, 196])\n",
      "torch.Size([2, 512, 196])\n",
      "torch.Size([2, 512, 196])\n",
      "torch.Size([2, 512, 196])\n",
      "torch.Size([2, 512, 196])\n",
      "torch.Size([2, 512, 196])\n",
      "torch.Size([2, 512, 196])\n",
      "torch.Size([2, 512, 196])\n",
      "torch.Size([2, 1500, 196])\n",
      "torch.Size([2, 1500, 196])\n",
      "torch.Size([2, 1500, 196])\n",
      "torch.Size([2, 1500, 196])\n",
      "torch.Size([2, 3000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0794,  0.1774,  0.4354, -0.0005],\n",
       "        [-0.2210,  0.2436,  0.2032, -0.1710]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgenderTDNNv1(90, 4)(torch.Tensor([x_train[0], x_train[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0468, -0.1723,  0.3643,  0.0596],\n",
       "        [-0.1401, -0.2256,  0.3341,  0.1411],\n",
       "        [-0.2596, -0.3465,  0.2531,  0.2095],\n",
       "        [-0.2305, -0.0442,  0.3455,  0.1286],\n",
       "        [-0.2236, -0.3733,  0.3789,  0.2213],\n",
       "        [-0.3489, -0.0838,  0.3531,  0.0782],\n",
       "        [-0.2147, -0.2110,  0.2759,  0.1107],\n",
       "        [-0.1037, -0.0840,  0.2893,  0.2697]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AgenderTDNNv0\n",
    "# AgenderTDNNv1\n",
    "# AgenderCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture output\n",
    "\n",
    "define_seed(12)\n",
    "# model = AgenderResNet50(len(class_names), requires_grad=True)\n",
    "model = AgenderTDNNv2(90, len(class_names))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.0001) # v1\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.01)\n",
    "    \n",
    "model, max_epoch, max_recall = train_model(model, loss, optimizer, scheduler, num_epochs=100, \n",
    "                                           device=device, \n",
    "                                           train_dataloader=train_dataloader, \n",
    "                                           valid_dataloader=valid_dataloader,\n",
    "                                           class_names=class_names,\n",
    "                                           log_root=log_root,\n",
    "                                           tb_log_root=tb_log_root,\n",
    "                                           features_name=features_name,\n",
    "                                           experiment_name='AgenderTDNNv2_adam100',\n",
    "                                           log_iter=[])\n",
    "    \n",
    "print('Epoch: {0}, maximum recall: {1}\\n'.format(max_epoch, max_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_test_predictions = []\n",
    "all_test_labels = []\n",
    "\n",
    "model_name = 'mel_128_AgenderResNet50_adam8'\n",
    "epoch = 7\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "    \n",
    "dictionary_path = get_model_by_epoch(os.path.join(log_root, model_name), epoch)\n",
    "print(dictionary_path)\n",
    "checkpoint = torch.load(dictionary_path)\n",
    "    \n",
    "model = AgenderResNet50(len(class_names), requires_grad=True)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "    \n",
    "for inputs, labels in train_dataloader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    predicts = None\n",
    "    with torch.set_grad_enabled(False):\n",
    "        preds = model(inputs)\n",
    "        \n",
    "    predicts = torch.nn.functional.softmax(preds, dim=1).data.cpu().numpy()\n",
    "    \n",
    "    all_labels.append(labels.data.cpu().numpy())\n",
    "    all_predictions.append(predicts)\n",
    "        \n",
    "all_labels = np.concatenate(fold_labels)\n",
    "all_predictions = np.concatenate(fold_predictions)\n",
    "    \n",
    "all_recall = recall_score(all_labels, np.argmax(all_predictions, axis=1), average='macro')\n",
    "print('Train Recall: {:.4f}'.format(all_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, splits):\n",
    "    res = np.concatenate((all_test_predictions[i], np.expand_dims(all_test_labels[i], axis=1)), axis=1)\n",
    "    np.savetxt(\"SDResNet34_cv_valid_preds_{}.csv\".format(i), res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "all_predictions = []\n",
    "    \n",
    "dictionary_path = get_model_by_epoch(os.path.join(log_root, model_name), epoch)\n",
    "print(dictionary_path)\n",
    "checkpoint = torch.load(dictionary_path)\n",
    "    \n",
    "model = AgenderResNet50(len(class_names), requires_grad=True)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "    \n",
    "for inputs, labels in valid_dataloader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    predicts = None\n",
    "    with torch.set_grad_enabled(False):\n",
    "        preds = model(inputs)\n",
    "        \n",
    "    predicts = torch.nn.functional.softmax(preds, dim=1).data.cpu().numpy()\n",
    "    \n",
    "    all_labels.append(labels.data.cpu().numpy())\n",
    "    all_predictions.append(predicts)\n",
    "        \n",
    "all_labels = np.concatenate(fold_labels)\n",
    "all_predictions = np.concatenate(fold_predictions)\n",
    "    \n",
    "all_recall = recall_score(all_labels, np.argmax(all_predictions, axis=1), average='macro')\n",
    "print('Valid Recall: {:.4f}'.format(all_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, splits):\n",
    "    res = np.concatenate((all_test_predictions[i], np.expand_dims(all_test_labels[i], axis=1)), axis=1)\n",
    "    np.savetxt(\"SDResNet34_devel_preds_{}.csv\".format(i), res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_predictions = []\n",
    "all_test_labels = []\n",
    "\n",
    "get_model_name = lambda idx: 'mel_128_SDResNet_adam_3_4Fold'\n",
    "get_model_epoch = lambda idx: [1, 2, 0][idx]\n",
    "\n",
    "for i in range(0, splits):\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                                  batch_size=batch_size, \n",
    "                                                  shuffle=False, \n",
    "                                                  num_workers=6)\n",
    "    \n",
    "    fold_labels = []\n",
    "    fold_predictions = []\n",
    "    \n",
    "    dictionary_path = get_model_by_epoch(os.path.join(log_root, '{0}_{1}'.format(get_model_name(i), i)), get_model_epoch(i))\n",
    "    print(dictionary_path)\n",
    "    checkpoint = torch.load(dictionary_path)\n",
    "    \n",
    "#     model = SDNetB()\n",
    "    model = SDResNet(requires_grad=True)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        predicts = None\n",
    "        with torch.set_grad_enabled(False):\n",
    "            preds = model(inputs)\n",
    "        \n",
    "        predicts = torch.nn.functional.softmax(preds, dim=1).data.cpu().numpy()\n",
    "    \n",
    "        fold_labels.append(labels.data.cpu().numpy())\n",
    "        fold_predictions.append(predicts)\n",
    "        \n",
    "    fold_labels = np.concatenate(fold_labels)\n",
    "    fold_predictions = np.concatenate(fold_predictions)\n",
    "    \n",
    "    all_test_labels.append(fold_labels)\n",
    "    all_test_predictions.append(fold_predictions)\n",
    "    \n",
    "    test_recall = recall_score(fold_labels, np.argmax(fold_predictions, axis=1), average='macro')\n",
    "    print('Fold {}, Test Recall: {:.4f}'.format(i, test_recall))\n",
    "    print('Fold {} OK'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, splits):\n",
    "    res = np.concatenate((all_test_predictions[i], np.expand_dims(all_test_labels[i], axis=1)), axis=1)\n",
    "    np.savetxt(\"SDResNet34_test_preds_{}.csv\".format(i), res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_name = 'ComParE19_SD.{0}.test.IIAS_new.csv'.format(features_name)\n",
    "\n",
    "prepare_filenames = lambda x: '{0}.wav'.format(os.path.splitext(os.path.basename(x))[0])\n",
    "\n",
    "submission_df = pd.DataFrame.from_dict({'file_name': df_labels['file_name'][df_labels['file_name'].str.startswith('test')].values, \n",
    "                                        'prediction': le.inverse_transform(np.argmax(test_preds, axis=1))})\n",
    "submission_df.to_csv(pred_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv(pred_file_name)\n",
    "y_test_pred = df_pred['prediction'].values\n",
    "\n",
    "df_labels = pd.read_csv(os.path.join(data_root, labels_path))\n",
    "y_test = df_labels['label'][df_labels['file_name'].str.startswith('test')].values\n",
    "\n",
    "print('Test UAR: {0:.1f}'.format(recall_score(y_test, y_test_pred, labels=class_names, average='macro') * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
